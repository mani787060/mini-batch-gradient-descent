# Mini-Batch Gradient Descent

-> This project explains and implements **Mini-Batch Gradient Descent**, combining the advantages of both Batch GD and Stochastic GD.  
-> It includes a complete workflow of creating batches, calculating gradients, and updating model parameters.

## Contents:-
-> Mini-Batch Gradient Descent implemented **from scratch using NumPy**
-> Mini-Batch GD using **sklearn SGDRegressor**
-> Batch creation and shuffling
-> Loss reduction visualization across epochs

## Key Concepts
-> Difference between Batch, Stochastic, and Mini-Batch GD
-> How batch size affects stability and speed
-> Understanding convergence behavior

## Libraries Used
-> NumPy
-> Pandas
-> Matplotlib
-> Scikit-learn

